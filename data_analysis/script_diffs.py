# scrape all the scripts and compare

import sys
import os.path
import csv
import json
import bs4
from bs4 import BeautifulSoup
import base64
import difflib
from tabulate import tabulate
from collections import OrderedDict

# spatial comparison:
# want to scrape the scripts from each response body
# for each website, for each crawl_ID and run_ID

# get n sites from alexa top sites
def get_sites(n="301"):
    alexa_sites = []
    #print "Starting to get sites..."
    with open('top-1m.csv', 'r') as f:
        reader = csv.reader(f)
        r = list(reader)
        #print "Created reader..."
        for item in r:
            # change to 501 for Alexa 500
            if item[0] == n:
                break
            else:
                #print "Appending www."+item[1]
                alexa_sites.append("www."+item[1])
    return alexa_sites

# get list of nodes
def get_nodes():
    nodes = {} # Want to map node hostnames to codes for abbreviated printing
    f = open('/n/fs/multisurf/nodes.txt', 'r')
    count = 0
    for node in f:
        if not node.startswith('#'):
            count += 1
            node_strs = node.split(",")
            nodes[node_strs[0].strip()] = node_strs[1].strip()
    print("Getting script data from "+str(count)+" nodes")
    return nodes

# extracts the string value within each script tag and converts it to ascii
def extract_script_strs (scripts):
    script_strs = []

    for i in range (0, len(scripts)):
        script_str = str(scripts[i].string)
        script_strs.append(script_str)

    return script_strs

# read in all of the bodies for a given crawl and trial ID
# and scrape all of the scripts
def read_bodies_for_crawl (sites, nodes, run, crawl_id, trial_id):
    scripts_all_sites_all_nodes = OrderedDict()
    # for each site
    for i in range(0, len(sites)):
        # Want to preserve order of sites, so traverse the list in order
        site = sites[i]
        scripts_for_site = OrderedDict()
        
        # for each node
        for node in nodes:
            #print("Reading body for site "+site+" for node "+node)

            # if we have a file for the given node, crawl_id and trial_id
            fname = node+"_"+str(crawl_id)+"_"+str(trial_id)+"_"+"body_"+site

            if os.path.isfile("/n/fs/multisurf/"+run+"/"+fname):
                # get the entire response body
                f = open("/n/fs/multisurf/"+run+"/"+fname)
                body = base64.b64decode(f.read())
                f.close()
                # and scrape all of the scripts
                soup = BeautifulSoup(body, "html.parser")
                node_script_list = extract_script_strs(soup.find_all('script'))
                scripts_for_site[node] = len(node_script_list), node_script_list
        scripts_all_sites_all_nodes[site] = scripts_for_site
    return scripts_all_sites_all_nodes

# Organizes the number of scripts per site per node so this data can be printed
# as a table by tabulate
def organize_num_scripts (scripts_all_sites_all_nodes, nodes):
    nums = OrderedDict()
    nums['sites'] = []

    # set up the data structure for pretty printing later
    for node in nodes:
        node_code = nodes[node] # Use the node codes for abbreviated printing
        nums[node_code] = []

    # now go through the numbers
    for site in scripts_all_sites_all_nodes:
        scripts_for_site = scripts_all_sites_all_nodes[site]
        
        nums['sites'].append(site)

        for node in nodes:
            node_code = nodes[node]
            node_script_info = scripts_for_site.get(node)

            if node_script_info == None:
                nums[node_code].append("X")
            else:
                nums[node_code].append(node_script_info[0])

    return nums        

# checks if all elements in iterator are equal, stops if diff is found
def check_equal(iterator):
      try:
         iterator = iter(iterator)
         first = next(iterator)
         return all(first == rest for rest in iterator)
      except StopIteration:
         return True

# removes the sites for which the number of scripts is all the same
# Assumes input is in an orderedDict generated by organize_num_scripts
def remove_all_same_script_nums (nums, num_sites, nodes):
    nums_cleaned = nums

    # for each element (corresponding to each site) in each node's 
    # create a list of numbers so we can compare all elements for equality directly
    for i in range(0, num_sites):
        nums_for_site = []
        for node, code in nodes.items():
            num_list = nums_cleaned[code]
            print(i)
            nums_for_site.append(num_list[i])

        # now check if all elements are identical
        are_identical = check_equal(nums_for_site)

        if are_identical:
            # they are so this will be a redundant row in our table
            # remove this row from each mapping in the nums dict
            for key in nums:
                num_list = nums_cleaned[key]
                del num_list[i]

    return nums_cleaned

# remove "close matches" of scripts from the lists
'''def remove_close_matches (scripts_for_site):
    for node1 in scripts_for_site:
        for node2 in scripts_for_site:
            if node1 != node2:
  '''              

# now we compare each script for each site for each node
def compare_scripts (scripts_all_sites_all_nodes):
    scripts_diff = {}
    # loop over all keys (i.e. sites) in the scripts dict
    for site in scripts_all_sites_all_nodes:
        scripts_for_site = scripts_all_sites_all_nodes[site]
        site_scripts_diff = {}
        
        if len(scripts_for_site) == 0:
            scripts_diff[site] = "No scripts found"
        else:
            # now we compare each node with each other
            for node1 in scripts_for_site:
                (node1_num_scripts, node1_scripts) = scripts_for_site[node1]
                node1_diff = site_scripts_diff[node1] = {}
                
                for node2 in scripts_for_site:
                    # we don't want to compare ourselves with ourselves
                    if node1 != node2:
                        print("Comparing "+node1+" and "+node2+" for site "+site)
                        
                        (node2_num_scripts, node2_scripts) = scripts_for_site[node2]
                        
                        # this records all scripts that are in node2's view not in node1's list
                        diff = list((set(node2_scripts)).difference(set(node1_scripts)))

                        node1_diff[node2] = diff
            scripts_diff[site] = site_scripts_diff
    return scripts_diff

def usage():
     print("Usage: python script_diffs.py <data: c or s> <run_name> <crawl_id> <trial_id> <output filename> <num_sites>")
     exit()

''' Here's where the main script starts '''

if len(sys.argv) < 7:
   usage()

data_to_report = sys.argv[1]

if not (data_to_report == "c" or data_to_report == "s"):
    print("Must indicate data to report: either \"c\" for the script count diff or \"s\" for the script diff")
    usage()

run_name = sys.argv[2]
crawl_id = sys.argv[3]
trial_id = sys.argv[4]
outfile = sys.argv[5]
sites = get_sites(sys.argv[6])
nodes = get_nodes()
scripts = read_bodies_for_crawl(sites, nodes, run_name, crawl_id, trial_id)

out = open("/n/fs/multisurf/"+outfile, 'w+')

if data_to_report == "c":
    script_nums = organize_num_scripts(scripts, nodes)
    script_nums_clean = remove_all_same_script_nums(script_nums, len(sites), nodes)
    # print pretty table
    out.write(tabulate(script_nums_clean, headers="keys"))
    out.write("\n")
elif data_to_report == "s":
    script_diffs = compare_scripts(scripts)
    # print pretty JSON
    json.dump(script_diffs, out, sort_keys=True, indent=4, separators=(",", ":"))
    out.write("\n")

out.close()

                    
                                
                            
                        
                            
                            
                                
                                
                                
                            

# scrape all the scripts and compare

import sys
import os.path
import csv
import json
import bs4
from bs4 import BeautifulSoup
import base64
from difflib import SequenceMatcher
from tabulate import tabulate
from collections import OrderedDict

# spatial comparison:
# want to scrape the scripts from each response body
# for each website, for each crawl_ID and run_ID

# get n sites from alexa top sites
def get_sites(n="301"):
    alexa_sites = []
    #print "Starting to get sites..."
    with open('/n/fs/multisurf/top-1m.csv', 'r') as f:
        reader = csv.reader(f)
        r = list(reader)
        #print "Created reader..."
        for item in r:
            # change to 501 for Alexa 500
            if item[0] == n:
                break
            else:
                #print "Appending www."+item[1]
                alexa_sites.append("www."+item[1])
    return alexa_sites

# get list of nodes
def get_nodes():
    nodes = {} # Want to map node hostnames to codes for abbreviated printing
    f = open('/n/fs/multisurf/nodes.txt', 'r')
    count = 0
    for node in f:
        if not node.startswith('#'):
            count += 1
            node_strs = node.split(",")
            nodes[node_strs[0].strip()] = node_strs[1].strip()
    print("Getting script data from "+str(count)+" nodes")
    return nodes

# extracts the string value within each script tag and converts it to ascii
def extract_script_strs (scripts):
    script_strs = []

    for i in range (0, len(scripts)):
        script_str = str(scripts[i].string)
        script_strs.append(script_str)

    return script_strs

# read in all of the bodies for a given crawl and trial ID
# and scrape all of the scripts
def read_bodies_for_crawl (sites, nodes, run, crawl_id, trial_id):
    scripts_all_sites_all_nodes = OrderedDict()
    # for each site
    for i in range(0, len(sites)):
        # Want to preserve order of sites, so traverse the list in order
        site = sites[i]
        scripts_for_site = OrderedDict()
        
        # for each node
        for node in nodes:
            #print("Reading body for site "+site+" for node "+node)

            # if we have a file for the given node, crawl_id and trial_id
            fname = node+"_"+str(crawl_id)+"_"+str(trial_id)+"_"+"body_"+site

            if os.path.isfile("/n/fs/multisurf/"+run+"/"+fname):
                # get the entire response body
                f = open("/n/fs/multisurf/"+run+"/"+fname)
                body = base64.b64decode(f.read())
                f.close()
                # and scrape all of the scripts
                soup = BeautifulSoup(body, "html.parser")
                node_script_list = extract_script_strs(soup.find_all('script'))
                scripts_for_site[node] = len(node_script_list), node_script_list
        scripts_all_sites_all_nodes[site] = scripts_for_site
    return scripts_all_sites_all_nodes

# Organizes the number of scripts per site per node so this data can be printed
# as a table by tabulate
def organize_num_scripts (scripts_all_sites_all_nodes, nodes):
    nums = OrderedDict()
    nums['sites'] = []

    # set up the data structure for pretty printing later
    for node in nodes:
        node_code = nodes[node] # Use the node codes for abbreviated printing
        nums[node_code] = []

    # now go through the numbers
    for site in scripts_all_sites_all_nodes:
        scripts_for_site = scripts_all_sites_all_nodes[site]
        
        nums['sites'].append(site)

        for node in nodes:
            node_code = nodes[node]
            node_script_info = scripts_for_site.get(node)

            if node_script_info == None:
                nums[node_code].append("X")
            else:
                nums[node_code].append(node_script_info[0])

    return nums        

# checks if all elements in iterator are equal, stops if diff is found
def check_equal(iterator):
      try:
         iterator = iter(iterator)
         first = next(iterator)
         return all(first == rest for rest in iterator)
      except StopIteration:
         return True

# removes the sites for which the number of scripts is all the same
# Assumes input is in an orderedDict generated by organize_num_scripts
def remove_all_same_script_nums (nums, num_sites, nodes):
    nums_cleaned = nums

    to_delete_indices = []

    # for each element (corresponding to each site) in each node's 
    # create a list of numbers so we can compare all elements for equality directly
    for i in range(0, num_sites):
        # since we would actively shrink the length of the lists if we delete on the spot
        # let's collect all of the indices that need to be deleted a priori and then remove
        # all at once

        nums_for_site = []
        for node, code in nodes.items():
            num_list = nums_cleaned[code]
            nums_for_site.append(num_list[i])

        # now check if all elements are identical
        are_identical = check_equal(nums_for_site)

        if are_identical:
            # they are so this will be a redundant row in our table
            # so mark this index to be removed from all lists
            to_delete_indices.append(i)
 
    # now remove all indices in reverse order to we shrink the list from
    # back to front
    for i in reversed(to_delete_indices):
        for key in nums_cleaned:
            num_list = nums_cleaned[key]
            del num_list[i]

    return nums_cleaned

# remove "close matches" of scripts from the lists
'''def remove_close_matches (scripts_for_site):
    for node1 in scripts_for_site:
        for node2 in scripts_for_site:
            if node1 != node2:
  '''              

# get the unique scripts seen by each site
def get_unique_scripts (scripts_all_sites_all_nodes, nodes):
    unique_scripts = OrderedDict()
    # loop over all keys (i.e. sites) in the scripts dict
    for site in scripts_all_sites_all_nodes:
        scripts_for_site = scripts_all_sites_all_nodes[site]
        
        # now we compare each node with each other
        unique_scripts_for_site = OrderedDict()
        for node1 in nodes:
            node1_code = nodes[node1]
            node1_script_info = scripts_for_site.get(node1)

            # build a similarity mapping for each script seen by this node
            sim_map = OrderedDict()
            if node1_script_info != None and node1_script_info[0] > 0:
                for script1 in node1_script_info[1]:
                    sim_map[script1] = 0.0

                for node2 in nodes:
                    # right now this will only compare if we even have something to compare to
                    if node1 != node2:
                        node2_script_info = scripts_for_site.get(node2)

                        if node2_script_info != None and node2_script_info[0] > 0:
                            # both nodes see some number of scripts for this site

                            # compute the similarity ratio between each script in node1's list and node2's list
                            for script1 in sim_map:
                                for script2 in node2_script_info[1]:
                                    s = SequenceMatcher(None, script1, script2, autojunk=False)
                                    sim_map[script1] = s.quick_ratio()
                
            
            # now that we've compared all of node1's scripts to all of node2's scripts
            # add the ones that have a similarity measure less than 0.6 to our mapping of unique scripts
            # even if there is only one other site with a close match for a script, it's not considered unique
            unique_scripts_for_site[node1_code] = []
            for script1 in sim_map:
                if sim_map[script1] < 0.6:
                    unique_scripts_for_site[node1_code].append(script1)

            # if a node doesn't have any unique scripts, remove it from the dict
            if len(unique_scripts_for_site[node1_code]) == 0:
                del unique_scripts_for_site[node1_code]

        unique_scripts[site] = tabulate(unique_scripts_for_site, headers="keys")
    return unique_scripts

def usage():
     print("Usage: python script_diffs.py <data: c or u> <run_name> <crawl_id> <trial_id> <output filename> <num_sites>")
     exit()

''' Here's where the main script starts '''

if len(sys.argv) < 7:
   usage()

data_to_report = sys.argv[1]

if not (data_to_report == "c" or data_to_report == "u"):
    print("Must indicate data to report: either \"c\" for the script count diff or \"u\" for unique scripts")
    usage()

run_name = sys.argv[2]
crawl_id = sys.argv[3]
trial_id = sys.argv[4]
outfile = sys.argv[5]
sites = get_sites(sys.argv[6])
nodes = get_nodes()
scripts_all_sites_all_nodes = read_bodies_for_crawl(sites, nodes, run_name, crawl_id, trial_id)

out = open("/n/fs/multisurf/"+outfile, 'w+')

if data_to_report == "c":
    script_nums = organize_num_scripts(scripts_all_sites_all_nodes, nodes)
    script_nums_clean = remove_all_same_script_nums(script_nums, len(sites), nodes)
    # print pretty table
    out.write(tabulate(script_nums_clean, headers="keys"))
    out.write("\n")
elif data_to_report == "u":
    unique_scripts = get_unique_scripts(scripts_all_sites_all_nodes, nodes)
    # print pretty JSON
    json.dump(unique_scripts, out, indent=4, separators=(",", ":"))
    out.write("\n")

out.close()

                    
                                
                            
                        
                            
                            
                                
                                
                                
                            
